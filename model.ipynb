{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11520/3990189032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "import seaborn as sns\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import functools as f\n",
    "from sklearn.model_selection import train_test_split\n",
    "import splitfolders\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array \n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('../Alzheimer_classification/Dataset', output=\"output\", seed=1345, ratio=(.8, 0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Height and width in pixels\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "data_splits = []\n",
    "direct = ['output/train', 'output/test', 'output/val']\n",
    "for d in direct: \n",
    "    var_name = tf.keras.utils.image_dataset_from_directory(\n",
    "                    d,\n",
    "                    label_mode='categorical',\n",
    "                    labels='inferred',\n",
    "                    batch_size=64,\n",
    "                    image_size=(img_height, img_width),\n",
    "                    seed=123,\n",
    "                )\n",
    "    data_splits.append(var_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize images\n",
    "dem_types = ['Mild_Demented', 'Moderate_Demented', 'Non_Demented', 'Very_Mild_Demented']\n",
    "examples = ['Mild_Demented/mild_2.jpg', 'Moderate_Demented/moderate_2.jpg', 'Non_Demented/non_2.jpg', 'Very_Mild_Demented/verymild_2.jpg']\n",
    "for i in range(len(dem_types)): \n",
    "    plt.plot(128,128)\n",
    "    plt.figure(i+1) \n",
    "    plt.title(dem_types[i])\n",
    "    plt.xlabel(\"X pixel scaling\")\n",
    "    plt.ylabel(\"Y pixels scaling\")\n",
    "    plt.imshow(image.imread('Dataset/'+ examples[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5 CNN Architecture Modified (MaxPool and Dropout 25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 CNN Architecture Modified (MaxPool and Dropout 25%)\n",
    "model = keras.Sequential()\n",
    "model.add(Rescaling(1./255, input_shape=(128, 128, 3)))\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation = 'relu', input_shape = (128, 128, 3), kernel_initializer=\"he_normal\"))\n",
    "model.add(layers.MaxPooling2D())\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation = 'relu', input_shape = (128, 128, 3), kernel_initializer=\"he_normal\"))\n",
    "model.add(layers.MaxPooling2D())\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units = 128, activation = 'relu', kernel_initializer=\"he_normal\"))\n",
    "model.add(layers.Dense(units = 64, activation = 'relu'))\n",
    "model.add(layers.Dense(units = 4, activation = 'softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(data_splits[0], epochs = 20, validation_data = data_splits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy - LeNet5 Architecture Modified\")\n",
    "plt.plot(hist.history['accuracy'],c='b',label='train - accuracy')\n",
    "plt.plot(hist.history['val_accuracy'],c='r',label='validation - accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy - LeNet5 Architecture Modified')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print('Loss - LeNet5 Architecture Modified')\n",
    "plt.plot(hist.history['loss'],c='orange',label='train - loss')\n",
    "plt.plot(hist.history['val_loss'],c='g',label='validation - loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss - LeNet5 Architecture Modified')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(data_splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LeNet-5e50Mod.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('LeNet-5e50Mod.h5')\n",
    "preds = np.array([])\n",
    "labels =  np.array([])\n",
    "for x, y in data_splits[1]:\n",
    "    preds = np.concatenate([preds, np.argmax(model2.predict(x), axis = -1)])\n",
    "    labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(labels, preds))\n",
    "print(\"Sensitivity:\",metrics.recall_score(labels, preds, average='macro'))\n",
    "print(\"Precision:\",metrics.precision_score(labels, preds, average='macro', zero_division=1))\n",
    "print(\"Recall:\",metrics.recall_score(labels, preds, average='macro' ))\n",
    "print(\"F1 Score:\",f1_score(labels, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "Cm=confusion_matrix(labels, preds)\n",
    "ax=plt.axes()\n",
    "sns.heatmap(Cm, cmap='Blues' ,annot=True,\n",
    "           annot_kws={'size':10},\n",
    "           ax=ax\n",
    "           )\n",
    "\n",
    "plt.title(\"Confusion Metrics for Alzheimer Disease Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5 CNN Architecture (with MaxPooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LeNet-5 CNN Architecture (with MaxPooling)\n",
    "model2 = keras.Sequential()\n",
    "model2.add(Rescaling(1./255, input_shape=(128, 128, 3)))\n",
    "model2.add(Conv2D(filters=6, kernel_size=(3, 3), padding='same', activation = 'relu', kernel_initializer=\"he_normal\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation = 'relu', kernel_initializer=\"he_normal\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model2.add(Dropout(0.20))\n",
    "\n",
    "model2.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=\"he_normal\"))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units = 128, activation = 'relu', kernel_initializer=\"he_normal\"))\n",
    "model2.add(Dense(units = 64, activation = 'relu'))\n",
    "model2.add(Dense(units = 4, activation = 'softmax'))\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "visualkeras.layered_view(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = model2.fit(data_splits[0], epochs = 20, validation_data = data_splits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy - LeNet-5 with MaxPooling\")\n",
    "plt.plot(hist2.history['accuracy'],c='b',label='train - accuracy')\n",
    "plt.plot(hist2.history['val_accuracy'],c='r',label='validation - accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print('Loss - LeNet-5 with MaxPooling')\n",
    "plt.plot(hist2.history['loss'],c='orange',label='train - loss')\n",
    "plt.plot(hist2.history['val_loss'],c='g',label='validation - loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(data_splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('LeNet-5Maxe50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('LeNet-5Maxe50.h5')\n",
    "preds = np.array([])\n",
    "labels =  np.array([])\n",
    "for x, y in data_splits[1]:\n",
    "    preds = np.concatenate([preds, np.argmax(model2.predict(x), axis = -1)])\n",
    "    labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(labels, preds))\n",
    "print(\"Sensitivity:\",metrics.recall_score(labels, preds, average='macro'))\n",
    "print(\"Precision:\",metrics.precision_score(labels, preds, average='macro', zero_division=1))\n",
    "print(\"Recall:\",metrics.recall_score(labels, preds, average='macro' ))\n",
    "print(\"F1 Score:\",f1_score(labels, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "Cm=confusion_matrix(labels, preds)\n",
    "ax=plt.axes()\n",
    "sns.heatmap(Cm, cmap='Blues' ,annot=True,\n",
    "           annot_kws={'size':10},\n",
    "           ax=ax\n",
    "           )\n",
    "\n",
    "plt.title(\"Confusion Metrics for Alzheimer Disease Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Architecture\n",
    "model3 = Sequential()\n",
    "model3.add(Rescaling(1./255, input_shape=(img_height, img_width, 3)))\n",
    "model3.add(Conv2D(filters=16,kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=\"he_normal\"))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model3.add(Conv2D(filters=32,kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=\"he_normal\"))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model3.add(Dropout(0.20))\n",
    "\n",
    "model3.add(Conv2D(filters=64,kernel_size=(3,3),padding='same',activation='relu',kernel_initializer=\"he_normal\"))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128,activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "model3.add(Dense(64,\"relu\"))\n",
    "model3.add(Dense(4,\"softmax\"))\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer = 'adam',metrics=[\"accuracy\"])\n",
    "visualkeras.layered_view(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist3 = model3.fit(data_splits[0], epochs = 20validation_data = data_splits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy - CNN Model\")\n",
    "plt.plot(hist3.history['accuracy'],c='b',label='train - accuracy')\n",
    "plt.plot(hist3.history['val_accuracy'],c='r',label='validation - accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print('Loss - CNN Model')\n",
    "plt.plot(hist3.history['loss'],c='orange',label='train - loss')\n",
    "plt.plot(hist3.history['val_loss'],c='g',label='validation - loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.evaluate(data_splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('CNN_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = load_model('CNN_Model.h5')\n",
    "img = load_img(\"output/test/Very_Mild_Demented/verymild_1737.jpg\")\n",
    "img = img.resize((128, 128))\n",
    "img = img_to_array(img) \n",
    "\n",
    "img = img.reshape(-1,128, 128,3)\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(cnn.predict(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array([])\n",
    "labels =  np.array([])\n",
    "for x, y in data_splits[1]:\n",
    "    preds = np.concatenate([preds, np.argmax(cnn.predict(x), axis = -1)])\n",
    "    labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(labels, preds))\n",
    "print(\"Sensitivity:\",metrics.recall_score(labels, preds, average='macro'))\n",
    "print(\"Precision:\",metrics.precision_score(labels, preds, average='macro', zero_division=1))\n",
    "print(\"Recall:\",metrics.recall_score(labels, preds, average='macro' ))\n",
    "print(\"F1 Score:\",f1_score(labels, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "Cm=confusion_matrix(labels, preds)\n",
    "ax=plt.axes()\n",
    "sns.heatmap(Cm, cmap='Blues' ,annot=True,\n",
    "           annot_kws={'size':10},\n",
    "           ax=ax\n",
    "           )\n",
    "\n",
    "plt.title(\"Confusion Metrics for Alzheimer Disease Classification\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00d6cdc61ab24fbeb134f3169ff7bc24c1e1abb99dfd40223bfb49ff3658ffd3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
